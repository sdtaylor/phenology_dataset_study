%% Submissions for peer-review must enable line-numbering 
%% using the lineno option in the \documentclass command.
%%
%% Preprints and camera-ready submissions do not need 
%% line numbers, and should have this option removed.
%%
%% Please note that the line numbering option requires
%% version 1.1 or newer of the wlpeerj.cls file, and
%% the corresponding author info requires v1.2

\documentclass[fleqn,10pt,lineno]{wlpeerj} % for journal submissions
% \documentclass[fleqn,10pt]{wlpeerj} % for preprint submissions

%% \title{Testing the usability of large scale citizen science data for phenology modeling}
\title{Comparison of large scale citizen science data and long term study data for phenology modeling}
\author[1]{Shawn D. Taylor}
\author[2]{Ethan P. White}
\affil[1]{UFL}
\affil[2]{UFL}
\corrauthor[1]{Shawn D. Taylor}{shawntaylor@weecology.org}

% \keywords{Keyword1, Keyword2, Keyword3}

\begin{abstract}
\end{abstract}

\begin{document}

\flushbottom
\maketitle
\thispagestyle{empty}

\section*{Introduction}

Citizen science data is becoming increasingly important for ecological research, with the potential for significant impact in many fields \citep{dickinson2010, tulloch2013, kelling2009}. Citizen science efforts have helped document range shifts \citep{hitch2007}, study the effect of landscape fragmentation on amphibians \citep{cosentino2014}, and informed ecological theory of community composition \citep{locey2013}. A primary benefit of citizen science data is observations made at numerous sites across regional to continental scales. This helps researchers explain large scale patterns across natural gradients, but can also introduce new sources error because of observer skill, variation in sampling effort, and spatial bias \citep{dickinson2010}. These issues can be accounted for in models but first must be identified and quantified. As new citizen science datasets are made available, they must be evaluated for their own biases and limitations.  

The National Phenology Network (NPN) is an organization which accepts observations of phenology from volunteers throughout North America and makes the data freely available \citep{schwartz2012a}. Plant phenology is important for studies of ecosystem function, carbon dynamics, pollination, and community ecology \citep{richardson2013, cleland2007, tang2016}. Robust phenology models are necessary for forecasting the future state of the climate as well as the impacts on plant and animal communities. At the largest scale variation in the timing of spring leaf out and fall senescence account for a significant amount of error in the carbon budget of earth system models, which has implications for correctly accounting for biosphere-atmosphere feedbacks in long term forecasts \citep{richardson2012}. At local scales, because of species specific responses to temperature and precipitation, changes in phenology can alter the flower community available to pollinators and seed predators throughout the growing season \citep{diez2012, caradonna2014, ogilvie2017, theobald2017}. 

The NPN dataset has already being used to study, among many things, variation in oak phenology \citep{gerst2017}, large scale community phenology models \citep{melaas2016}, and forecasting long term phenology trends \citep{jeong2013}. While there has been several studies looking at best practices when using this dataset \citep{crimmins2017, gerst2016} no study to date has looked a the underlying validity of the data. With hundreds of observers across North America, the observer and sampling bias for species identification and the true date of phenological events is potentially high. Though volunteers can be very accurate at distinguishing different phenophases \citep{fuccillo2015} observations can still be made sporadically throughout the growing season, some years skipped, or visits to a particular site discontinued entirely. These biases could adversely affect process based phenology models, which are species specific and depend on high precision in the data \citep{chuine2000}.

The NPN dataset is complicated further by the spatial variation of the temperature response within species. Phenology models currently have trouble capturing the spatial variation in warming requirements \citep{garcia-mozo2008, xu2013, olsson2014, basler2016}. Most phenology models are process based and designed to reflect physological processes driving phenological events. Yet models parameterized using observations from one site rarely transfer to other sites, and models using pooled data from many sites do not transfer well to individual sites \citep{garcia-mozo2008, basler2016}. This variation within species can be driven by, among other factors, plasticity in phenology requirements, local adaptation, microclimates, plant age, or population density \citep{kramer1995,diez2012}. While this complicates further any testing of dataset validity, it's worth it because there is a large opportunity with this dataset for further study of phenology across spatial gradients. 

Many phenological studies in the past have relied upon datasets with observations collected at a single location over a long timeframe by a single group \citep{cook2012, roberts2015, clark2014}. These datasets are assumed to not suffer from the potential biases mentioned above, and many also have high temporal coverage of several decades. They are disadvantageous as the inference from the models can only be applied to species located at the site, and only to local climate variability. These datasets have been invaluable for ecology for several decades, but datasets with a larger spatial extent and more complete species pool will be needed to model the complex phenology dynamics of ecosystems \citep{richardson2012, diez2012, caradonna2014}. The NPN datasets has the potential to meet these needs, and to date no study has assessed how well NPN data compares to long term study phenology datasets. Here we will compare process based phenology models built using only the NPN data with models built with data from Long Term Study (LTS) datasets. The specific research questions are:
\begin{enumerate}
\item Do parameter estimates of species level models differ when they are built with the NPN dataset versus an LTS dataset?
\item Do estimates of phenological events differ between the NPN and LTS derived models?
\item How well do models built with the NPN dataset transfer to LTS observations, and vice verse?
\end{enumerate}


\section*{Methods}

\subsection*{Datasets}

The National Phenology Network collections began in 2009 and continue to the present. Observations are based on status-based monitoring, where observers answer “yes”, “no”, or “unsure” when asked if a plant has a specific phenological phase present \citep{denny2014}. We downloaded all NPN observations from 2009-2016 for the phenophases Breaking Leaf Buds, Breaking Needle Buds, Emerging Needles, and Open Flowers. These phenophases apply to deciduous broadleafs, evergreen conifers, pines, and all angiosperms, respectively. Hereafter we will refer to these as either Flowers for the Open Flower phenophase, or Leaves for all other phenophases. We used moderately strict subsetting of NPN phenology observations as outlined in \cite{crimmins2017}. First, "yes" observations for individual plants were only kept if they were preceded by a "no" observation within 30 days. Observations for Leaves past day of year (DOY) 172 and for Flowers past DOY 213 were dropped. Finally only species that had greater than 30 total observations were kept. \cite{crimmins2017} only kept observations that were preceded by a "no" within 15 days, and also grouped multiple observations at single sites in a year to a single observations. We used 30 days to allow for a greater number of species to be compared. We did not group together multiple individuals at a single site to better incorporate intra-site variability. In addition to these steps we also inferred the date of each phenophase as the midpoint between each "yes" observation and the preceding "no" observation. 

To compare against the NPN dataset we used four LTS datasets from North America (Table 1). Three major ecosystem types and 24 species are represented in these datasets. As all four have been collected by single groups we make the assumption that they are free of any bias in species identification or sampling effort. Observation metrics varied among these four datasets due to different protocols, so were first converted to binary "yes" and "no" observations for each phenophase (see supplement for details). As with the NPN dataset we infer the date for each phenophase as the midpoint between the first "yes" observation and most recent "no" observation. 

\renewcommand{\arraystretch}{2}\tabcolsep=5pt
\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    Dataset Name & Habitat &  Phenological Event\newline (Num. Species) & Reference \\ \hline
    Harvard & N.E. Deciduous\newline Forest & Budburst (16)\newline Flowering (15) & \citep{okeefe2000} \\
    Jornada & Chihuahuan Desert & Flowering (2) & ... \\
    HJ Andrews \newline Experimental Forest & N.W. Wet Coniferous \newline Forest & Budburst (6)\newline Flowering (6) & ... \\
    Hubbard Brook & N.E. Deciduous \newline Forest & Budburst (3) & ... \\
    \hline
    \end{tabular}
\end{center}

\subsection*{Models}

For each species/phenophase in each of the five datasets we fit a range of phenology models from a naive mean day of year estimate to a more complex two phase forcing model (Table 2). With the exception of two models (naive and linear) the general model form predicts that a phenological event will happen when sufficient forcing units, \(F^{*}\), accumulate from a particular start day of the year. The day forcing accumulation begins, $t_{1}$, can either be estimated or fixed. For the growing degree day model (GDD) forcing units are the degrees in Celsius above an estimated threshold. The Fixed GDD model uses the same form but has fixed values for start day (Jan 1) and temperature threshold (0C). The Alternating model has a variable number of required forcing units defined as function of the total number of days below 0C (NCD). The Uniforc model is similar to the GDD model but has forcing units derived a sigmoid function. 

The naive model uses only the mean DOY from all prior observations as the estimated DOY. The linear model uses a regression with the mean spring (Jan. - March) temperature as the only independent variable and DOY as the response variable. 

\begin{center}
{\def\arraystretch{2}\tabcolsep=5pt
    \begin{tabular}{ | l | c | c | p{1.3cm} | l |}
    \hline
    Name & DOY Estimator & Forcing Equations & Total\newline Parameters & Reference \\ \hline
    Naive & \( \overline{DOY} \) & - & 1 & - \\
    Fixed GDD &$\sum_{t=0}^{DOY}R_{f}(x_{t})\geq F* $  & $R_{f}(x_{t}) = min(T, 0)$ & 1 & - \\
    Linear & \( DOY = \beta_{1} + \beta_{2}T_{mean} \) & - & 2 & - \\
    GDD & $\sum_{t=t_{1}}^{DOY}R_{f}(x_{t})\geq F* $ & $ R_{f}(x_{t}) = max(T - T^{*}, 0) $  & 3 & - \\
    Alternating & $\sum_{t=0}^{DOY}R_{f}(x_{t})\geq a + be^{cNCD(t)} $ & $R_{f}(x_{t}) = min(T, 0) $ & 3 & \citep{cannell1983} \\
    Uniforc &  $\sum_{t=t_{1}}^{DOY}R_{f}(x_{t})\geq F* $ & $ R_{f}(x_{t}) = \frac{1}{1 + e^{b(x-c)}} $ & 4 & \citep{chuine2000} \\

    \hline
    \end{tabular}
    }
\end{center}

Parameterization was done using differential evolution to minimize the root mean square error (RMSE) of the estimated day of year of the phenological event. Differential evolution is a global optimization algorithm similar to simulated annealing, which has traditionally been used to fit complex phenology models \citep{storn1997, chuine2000}. Initial testing showed that differential evolution produced more consistent results than simulated annealing and other similar algorithms. Confidence intervals for parameters were obtained by bootstrapping, where individual models were re-fit 250 times using a random sample, with replacement, of the data. A random 20\% of all data was held out from model fitting for use in later evaluation. 

\subsection*{Analysis}

As described above, for each species and phenophase two sets of models were produced: one set of models parameterized using only NPN data, and one set parameterized using only LTS data. The distribution for each parameter in each model was then compared between the two sets using a Mann-Whitney test. This was chosen due to the majority of parameters having non-normal distributions derived from the bootstrapped estimates. Statistically significant differences in these parameters, which are designed to represent biological mechanisms, would imply that the LTS and NPN datasets do not agree on

Even models with different structures can produce similar estimates of phenological events \citep{basler2016}. Thus we also compared estimates between LTS and NPN derived models using the root mean square difference (RMSD) between the two estimates:

$$ RMSD = \sqrt[]{ \frac{\sum_{i=1}^{n}(\widehat{DOY_{NPN,i}} - \widehat{DOY_{LTS,i}})^{2}}{n}} $$

where $n$ is the number of estimates produced for a single species and phenophase, and $LTS_{i}$ and $NPN_{i}$ are DOY estimates produced from the respective models. A RMSD value close to 0 implies that the LTS and NP derived models produced very similar estimates. For each species and phenophase 12 RMSD values were calculated from the 6 phenology models as well as 2 spatial extents. The first spatial extent used estimates from a the localities of the respective species in the NPN dataset, thus representing a large spatial extent. The second used estimates from the localities of the LTS datasets, thus representing a single site. The RMSD does not consider actual observations as it is meant to infer the correspondence of the NPN and LTS datasets beyond parameter comparison. 

For all models the actual error was calculated using the using the RMSE:

$$ RMSE = \sqrt[]{ \frac{\sum_{i=1}^{n}(\widehat{DOY_{i}} - DOY_{i})^{2}}{n}} $$

For each species and phenophase this was calculated for all combinations of LTS and NPN derived models, held out NPN and LTS observations, and the six phenology models. 

Pairwise model error comparison

$$ RMSE_{difference} = \widehat{RMSE_{NPN,s}} - RMSE_{LTS,s} $$

for all $$S = s_{1},...,s_{39}$$ species

\begin{figure}[]
	\centering
		\includegraphics[width=1\textwidth]{fig_1_param_comparison.png}
	\caption{Comparisons of parameters between NPN and LTS derived models. Each point represents a parameter value for a specific species/phenophase, and is the mean value from 250 bootstrapped  models. The black line is a 1:1 line.}
\end{figure}


\section*{Results}

All parameters had statistically different distributions between LTS and NPN derived estimates when tested with a Mann-Whitney U test. Visually comparing the mean values of parameters among all species and phenophases showed a correspondence among the simple models with 1-2 parameters (Fig. 1). As model complexity increased this correspondence between the two datasets decreased. The Naive model showed a distinct late bias in the mean DOY for phenologcial events, likely resulting from the LTS datasets being mostly in the northern United States. Though it still has a small bias the Fixed GDD model had the best correspondence between the two datasets (see suppliment for zoomed in figure). The outlier for this model, \textit{Larrea tridentata}, has phenology largely driven by precipitation, which is not considered in the Fixed GDD model \citep{beatley1974}. 

\begin{figure}
	\centering
		\includegraphics[width=1\textwidth]{fig_2_estimate_compare.png}
	\caption{Difference between NPN and LTS derived estimates. Each point is, for a single species/phenophase, the root mean square difference between LTS and NPN derived estimates for a specified model. A shows the RMSD over the extent of the NPN dataset, B shows the RMSD at the single sites represented by the LTS datasets.}
\end{figure}

When comparing estimates of phenological events between the two sets of models, many NPN models produced similar estimates to LTS models (Fig. 2). When estimating at local LTS sites, NPN models produced estimates within 10 days, on average, of LTS models except when using the Naive model (Fig. 2b). When estimating across the spatial extent of NPN data, except when using the Linear and Naive model, most NPN and LTS models produced estimates within 20 days of each other (Fig. 2a). 

Figure 3 shows the errors for all different model types when tested against the held out data. Models predicting NPN observations (Fig. 3a) have higher error rates than predictions for LTS observations (Fig. 3b). For the former models derived from NPN data made the best predictions, while LTS derived models performed best for the latter. Figure 4 shows the pairwise comparison of LTS and NPN errors. Similar to Figure 3, the NPN models performed best on NPN observations and LTS models performed best on LTS observations.

\begin{figure}
	\centering
		\includegraphics[width=1\textwidth]{fig_3_error_compare.png}
	\caption{Root mean square error of model predictions compared to observations. The top plot is for all held out observations in the NPN dataset. The bottom plot is for all held out observations in the LTS datasets.}
\end{figure}

\begin{figure}
	\centering
		\includegraphics[width=1\textwidth]{fig_4_pairwise_error.png}
	\caption{Density plots for pairwise RMSE comparison. The x axis shows the distribution for $RMSE_{NPN} - RMSE_{LTS}$  for the  39 species/phenophase combinations.}
\end{figure}

-Even with different parameter values, some species/models had extremely similar
estimates, suggesting these models had 

\section*{Discussion}

Despite having statistically different parameters to LTS derived models, many NPN derived models were able to produce similar estimates for phenological events. This is (likely) due to models having a non-linear parameter space and being non-identifiable. For example two GDD models with parameters of $t_{1}$=1, $F$=10, $T*$=0 and $t_{1}$=5, $F$=5, $T*$=0 can produce nearly identical results in many, but not all, scenarios. As model complexity increases the ability to produce similar estimates increases. Indeed, the Uniforic models with NPN derived parameters still had estimates within 10 days of LTS derived parameters for most species (Fig. 2) despite having very different parameters (Fig. 1). 

This makes the interpretation of these parameters questionable, which is troublesome as they are designed to be biologically relevant. \cite{chuine2016} found similar issues, where models estimating endodormancy break using only observed budbreak did not match direct measurements of endodormancy break, suggesting that phenology models which are fit using only the observed phenophase cannot accurately infer the internal mechanisms described in the models. \cite{basler2016} suggests that the thermal forcing in models is responsible for most of the accuracy, while any additional parameters are only fit to residuals and sensitive to over fitting. Here our simplest model, the Fixed GDD model which uses only a warming component, had reasonable (thought statistically different) correspondence between LTS and NPN datasets. This correspondence decreases as more parameters are added to models, suggesting that if the NPN dataset is being used to make inferences about plant physology simple models could give a more consistent results. Parameter estimates of more complex models do not agree with models from the same species in LTS datasets, and it is possible even the LTS datasets are over fitting these models. 

Even with similar parameters, the Fixed GDD model was not the best performing model when tested on out of sample data. While the values of additional parameters may be incorrect in the above sense, they nonetheless provide more predictive power. Across the two spatial scales the GDD, Uniforc, and Alternating model were generally the best performing (Fig. 3). Considering only NPN derived models predicting out of sample NPN observations the GDD and Uniforc models were the best performing across all species and phenophases, and commonly had nearly identical error values (Fig S1). One goal of ecological forecasting is to inform decision makers about the future state of systems (dietz?). Thus the best performing model should be used in these cases to give the best possible forecast. The GDD and Uniforc models have three and four parameters, respectively. Thus the simpler GDD model shows promise for building models with the NPN dataset and making forecasts across large scales.

Other models show promise for forecasting under different scenarios. For example if using data from a large spatial extent to make predictions at single sites our results show the Fixed GDD model is the best option, as performance under this scenario is very similar to performance of Fixed GDD models built from LTS data (Fig. 4). In the opposite scenario of making large scale predictions using only data from a single site the Alternating model is likely the best option, as most LTS derived models have a RMSE only 5 days greater on average than the corresponding NPN model when tested on out of sample NPN data (Fig. 4). The Alternating model is also one of the best performing model overall for large scale NPN data (Fig. 3a). 

The spatially corrected NPN models were essentially identical to the non-corrected ones in terms of parameters, estimates of phenological events, and out of sample error rates. Spatial variation in phenological requirements is known to exist in plants \citep{zhang2017}, yet models which attempt to model this variation did not significatnly improve estimates 

\section*{Conclusion}
Despite being collected across a large spatial extend and by hundreds of observers the NPN dataset can be used to build phenology models which make comparable predictions to LTS datasets. Indeed, if making predictions across a large spatial extent the NPN derived models are preferable to LTS derived models. If using the NPN dataset to make inferences about plant physiology researchers should not to use overly complicated models, as parameter estimates from complex NPN derived models do not agree with LTS derived models. Whether this disagreement stems from the spatial variation in plant phenology requirements, or in the irregular sampling of the NPN dataset is an avenue for future study. 

The National Phenology Network dataset will likely be an important resource for studying phenology.  The addition of numerous species and observations from across a range of conditions is important for informing large scale processes, as similar datasets have shown in Europe \citep{olsson2014, basler2016} and Asia \citep{xu2013, zhang2017}. Research from long term small scale studies has been invaluable and should no doubt be continued, but can now be complimented by using large scale NPN data (ie. \citep{jeong2013, melaas2016}). As long term phenology observations from the National Ecological Observatory Network \citep{elmendorf2016} become available in the coming years  ....


\section*{Acknowledgments}


\bibliography{refs}

\end{document}
